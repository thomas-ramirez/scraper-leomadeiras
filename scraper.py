import os, re, json, time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from pathlib import Path
from requests.adapters import HTTPAdapter, Retry
from contextlib import contextmanager

try:
    # Playwright √© opcional; usado para p√°ginas din√¢micas (ex.: Colcci)
    from playwright.sync_api import sync_playwright
except Exception:
    sync_playwright = None

downloads_path = str(Path.home() / "Downloads")
current_dir = os.path.dirname(os.path.abspath(__file__))  # Pasta atual do script

input_csv = os.path.join(current_dir, "data", "csv", "produtos_link.csv")
output_csv = os.path.join(current_dir, "data", "exports", "produtos_vtex.csv")
output_folder = os.path.join(current_dir, "data", "exports", "imagens_produtos")

df_links = pd.read_csv(input_csv)
if "url" not in df_links.columns:
    raise Exception("‚ùå A planilha precisa ter uma coluna chamada 'url'.")

os.makedirs(output_folder, exist_ok=True)

# === Sess√£o HTTP robusta ===
UA = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
    "Accept-Encoding": "identity",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
}
session = requests.Session()
session.headers.update(UA)
session.mount(
    "https://",
    HTTPAdapter(max_retries=Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])),
)

# === Mapeamentos VTEX (IDs) ===
maps = {
    "departamento": {
        "Eletrodom√©sticos": "1",
        "Eletroport√°teis": "2", 
        "Ar Condicionado": "3",
        "Aquecimento": "4",
        "Ventila√ß√£o": "5",
        "Refrigera√ß√£o": "6",
        "Lavagem": "7",
        "Cozinha": "8",
        "Limpeza": "9",
        "Pequenos Eletrodom√©sticos": "10",
        # Adicione mais conforme necess√°rio
    },
    "categoria": {
        "Frigobar": "1",
        "Freezer": "2",
        "Refrigerador": "3",
        "Ar Condicionado": "4",
        "Ventilador": "5",
        "Aquecedor": "6",
        "M√°quina de Lavar": "7",
        "Secadora": "8",
        "Fog√£o": "9",
        "Microondas": "10",
        "Liquidificador": "11",
        "Mixer": "12",
        "Processador": "13",
        "Aspirador": "14",
        "Ferro de Passar": "15",
        # Adicione mais conforme necess√°rio
    }
}

# Dicion√°rio para mapeamento din√¢mico de marcas
marca_mapping = {}
marca_counter = 1

# === Utils ===
def limpar(t):
    return re.sub(r"\s+", " ", (t or "").strip())

def get_marca_id(marca_nome):
    """Retorna o ID da marca no formato 2000XXX"""
    global marca_mapping, marca_counter
    
    if not marca_nome:
        return "2000001"  # Default
    
    marca_upper = marca_nome.upper().strip()
    
    if marca_upper not in marca_mapping:
        marca_mapping[marca_upper] = f"2000{marca_counter:03d}"
        marca_counter += 1
    
    return marca_mapping[marca_upper]

def parse_preco(texto):
    # 1¬™ ocorr√™ncia de R$ xxx,xx
    m = re.search(r"R\$\s*([\d\.\s]+,\d{2})", texto)
    if not m:
        return ""
    br = m.group(1).replace(".", "").replace(" ", "").replace(",", ".")
    try:
        return f"{float(br):.2f}"
    except:
        return ""

def get_next_data(soup):
    tag = soup.find("script", id="__NEXT_DATA__", type="application/json")
    if not tag:
        return None
    try:
        return json.loads(tag.string)
    except:
        return None

def get_jsonld(soup):
    for s in soup.find_all("script", type="application/ld+json"):
        try:
            data = json.loads(s.string)
            seq = data if isinstance(data, list) else [data]
            for it in seq:
                if isinstance(it, dict) and it.get("@type") in ("Product", "Offer", "AggregateOffer"):
                    return it
        except:
            continue
    return None

def parse_srcset(srcset):
    # retorna a URL com maior resolu√ß√£o do srcset
    # ex: "https://a.jpg 1x, https://b.jpg 2x" -> "https://b.jpg"
    if not srcset:
        return ""
    
    parts = srcset.split(",")
    best_url = ""
    best_density = 0
    
    for part in parts:
        part = part.strip()
        if ' ' in part:
            url_part, density_part = part.rsplit(' ', 1)
            try:
                # Remove 'x' e converte para float
                density = float(density_part.replace('x', '').replace('w', ''))
                if density > best_density:
                    best_density = density
                    best_url = url_part.strip()
            except:
                # Se n√£o conseguir parsear, usa a primeira URL
                if not best_url:
                    best_url = url_part.strip()
    
    return best_url if best_url else parts[0].strip().split(" ")[0].strip()

@contextmanager
def _playwright_context():
    if not sync_playwright:
        raise RuntimeError("Playwright n√£o est√° dispon√≠vel. Instale com: pip install playwright e python -m playwright install chromium")
    p = sync_playwright().start()
    try:
        yield p
    finally:
        p.stop()

def renderizar_html(url, wait_selectors=None, timeout_ms=15000):
    """Renderiza p√°gina via Chromium headless e retorna HTML.
    wait_selectors: lista de seletores CSS a esperar (qualquer um).
    """
    wait_selectors = wait_selectors or []
    with _playwright_context() as p:
        browser = p.chromium.launch(headless=True)
        try:
            context = browser.new_context()
            page = context.new_page()
            page.set_default_timeout(timeout_ms)
            page.goto(url, wait_until="domcontentloaded")
            # tentar aguardar rede ociosa e algum seletor de conte√∫do
            try:
                page.wait_for_load_state("networkidle", timeout=timeout_ms)
            except Exception:
                pass
            for sel in wait_selectors:
                try:
                    page.wait_for_selector(sel, state="attached", timeout=4000)
                    break
                except Exception:
                    continue
            return page.content()
        finally:
            browser.close()

def gerar_base_url_produto(sku, nome):
    """Gera baseUrl √∫nico para cada produto baseado no SKU e nome"""
    # Limpar nome para URL segura
    nome_limpo = re.sub(r'[^a-zA-Z0-9\s-]', '', nome).strip()
    nome_limpo = re.sub(r'\s+', '-', nome_limpo).lower()
    
    # Gerar baseUrl no formato: images-{leadPOC}-{sku}-{nome}
    base_url = f"images-leadPOC-{sku}-{nome_limpo}"
    return base_url

def baixar_imagem(url_img, fname):
    try:
        # Adicionar headers para tentar obter imagem em melhor qualidade
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'identity',
            'Referer': 'https://www.koerich.com.br/',
        }
        
        with session.get(url_img, headers=headers, stream=True, timeout=30) as resp:
            resp.raise_for_status()
            
            # Verificar se √© realmente uma imagem
            content_type = resp.headers.get('content-type', '')
            if not content_type.startswith('image/'):
                print(f"‚ö†Ô∏è URL n√£o √© uma imagem: {content_type}")
                return False
            
            # Verificar tamanho da imagem
            content_length = resp.headers.get('content-length')
            if content_length:
                size_kb = int(content_length) / 1024
                if size_kb < 1:  # Menos de 1KB, provavelmente thumbnail
                    print(f"‚ö†Ô∏è Imagem muito pequena ({size_kb:.1f}KB), pode ser thumbnail")
            
            with open(os.path.join(output_folder, fname), "wb") as f:
                for chunk in resp.iter_content(8192):
                    if chunk:
                        f.write(chunk)
        
        # Verificar se o arquivo foi criado e tem tamanho m√≠nimo
        file_path = os.path.join(output_folder, fname)
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            if file_size < 1024:  # Menos de 1KB
                print(f"‚ö†Ô∏è Arquivo muito pequeno ({file_size} bytes): {fname}")
                return False
        
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao baixar imagem {url_img}: {e}")
        return False

# === Core ===
def extrair_produto(url):
    # Koerich entrega conte√∫do via JS; usar Playwright para renderizar
    if "koerich.com.br" in url:
        try:
            html = renderizar_html(
                url,
                wait_selectors=[
                    "h1", 
                    ".product-name",
                    ".product-price",
                    ".product-images",
                    ".about-product",
                    ".specifications"
                ],
                timeout_ms=30000,
            )
        except Exception as e:
            print(f"‚ö†Ô∏è Erro com Playwright para {url}: {e}")
            # fallback: tentar HTML n√£o renderizado
            r = session.get(url, timeout=20)
            r.raise_for_status()
            html = r.text
        soup = BeautifulSoup(html, "html.parser")
    else:
        r = session.get(url, timeout=15)
        r.raise_for_status()
        soup = BeautifulSoup(r.content, "html.parser")

    # --- JSON-LD (priorit√°rio para nome/descri√ß√£o/pre√ßo/imagens) ---
    jsonld = get_jsonld(soup) or {}
    nome = limpar(jsonld.get("name")) if jsonld else ""
    descricao = limpar(jsonld.get("description")) if jsonld else ""
    preco = ""
    imgs = []

    # Imagens do JSON-LD (string ou lista)
    if isinstance(jsonld, dict) and jsonld.get("image"):
        if isinstance(jsonld["image"], list):
            for img in jsonld["image"]:
                if isinstance(img, str):
                    imgs.append(img)
        elif isinstance(jsonld["image"], str):
            imgs.append(jsonld["image"])

    # --- __NEXT_DATA__ (Next.js) para imagens de alta e campos internos ---
    nd = get_next_data(soup)
    if nd:
        def find_images(obj):
            found = []
            if isinstance(obj, dict):
                # chaves comuns em projetos Next/VTEX
                if "imageUrl" in obj and isinstance(obj["imageUrl"], str):
                    found.append(obj["imageUrl"])
                if "images" in obj and isinstance(obj["images"], list):
                    for it in obj["images"]:
                        if isinstance(it, dict) and "imageUrl" in it and isinstance(it["imageUrl"], str):
                            found.append(it["imageUrl"])
                for v in obj.values():
                    found.extend(find_images(v))
            elif isinstance(obj, list):
                for v in obj:
                    found.extend(find_images(v))
            return found

        next_images = find_images(nd)
        imgs += next_images

        # Nome (fallback a partir do Next)
        if not nome:
            try:
                prod = nd["props"]["pageProps"]["product"]
                for key in ("productName", "name", "title"):
                    if prod.get(key):
                        nome = limpar(prod[key])
                        break
            except Exception:
                pass

    # --- HTML Fallbacks (Koerich/MercadoCar) ---
    # Nome do produto
    if not nome:
        for sel in [".product-name h1", "h1.product-name", "h1", ".product-title"]:
            tag = soup.select_one(sel)
            if tag and tag.get_text(strip=True):
                nome = limpar(tag.get_text(strip=True))
                break
        if not nome:
            nome = "Sem Nome"

    # Pre√ßo: JSON-LD (offers.price) -> regex HTML
    if not preco and isinstance(jsonld, dict):
        offers = jsonld.get("offers") or {}
        if isinstance(offers, dict) and offers.get("price"):
            try:
                preco = f"{float(str(offers['price']).replace(',', '.')):.2f}"
            except:
                pass
    if not preco:
        preco = parse_preco(soup.get_text(" ", strip=True))

    # Descri√ß√£o: classes espec√≠ficas Koerich + fallbacks
    if not descricao:
        # Koerich: procurar se√ß√µes espec√≠ficas
        if "koerich.com.br" in url:
            # Se√ß√£o "Sobre o Produto"
            about_section = soup.select_one(".about-product")
            if about_section:
                descricao = limpar(about_section.get_text(" ", strip=True))
            
            # Se n√£o encontrou, procurar especifica√ß√µes
            if not descricao:
                specs_section = soup.select_one(".specifications")
                if specs_section:
                    descricao = limpar(specs_section.get_text(" ", strip=True))
            
            # Fallback: procurar por classes gen√©ricas de descri√ß√£o
            if not descricao:
                for sel in [".product-description", ".description", ".product-details", ".product-info"]:
                    tag = soup.select_one(sel)
                    if tag and tag.get_text(strip=True):
                        descricao = limpar(tag.get_text(" ", strip=True))
                        break
        
        # Fallback gen√©rico para outros sites
        if not descricao:
            for sel in [".full-description", ".product-description", ".descriptions-text", ".productDetails", ".descricao"]:
                tag = soup.select_one(sel)
                if tag and tag.get_text(strip=True):
                    descricao = limpar(tag.get_text(" ", strip=True))
                    break
        


        # --- Breadcrumb Schema.org -> Departamento/Categoria ---
    NomeDepartamento = ""
    NomeCategoria = ""
    
    # Para Koerich, buscar especificamente na div .category
    if "koerich.com.br" in url:
        category_div = soup.find("div", class_="category")
        if category_div:
            breadcrumb_ul = category_div.find("ul", id="breadcrumbTrail")
            if breadcrumb_ul:
                breadcrumb_items = breadcrumb_ul.find_all("li")
                breadcrumb_names = []
                
                for item in breadcrumb_items:
                    # Procurar por links ou texto
                    link = item.find("a")
                    if link:
                        text = limpar(link.get_text())
                        if text:
                            breadcrumb_names.append(text)
                    else:
                        text = limpar(item.get_text())
                        if text and text.lower() not in ("voc√™ est√° em:", "you are in:"):
                            breadcrumb_names.append(text)
                
                # Filtrar breadcrumbs v√°lidos (remover "Home", "In√≠cio", etc.)
                breadcrumb_names = [name for name in breadcrumb_names if name and name.lower() not in ("in√≠cio", "inicio", "home", "p√°gina inicial")]
                
                # Para Koerich, o √∫ltimo item √© o nome do produto, n√£o a categoria
                # Pegar o pen√∫ltimo como categoria e o antepen√∫ltimo como departamento
                if len(breadcrumb_names) >= 3:
                    NomeDepartamento = breadcrumb_names[-3]  # Antepen√∫ltimo item (ex: "Eletrodom√©sticos")
                    NomeCategoria = breadcrumb_names[-2]     # Pen√∫ltimo item (ex: "Fog√£o")
                elif len(breadcrumb_names) == 2:
                    NomeDepartamento = breadcrumb_names[0]   # Primeiro item
                    NomeCategoria = breadcrumb_names[1]      # Segundo item
                elif len(breadcrumb_names) == 1:
                    NomeCategoria = breadcrumb_names[0]
    
    # Se n√£o encontrou breadcrumb espec√≠fico do Koerich, tentar schema.org
    if not NomeDepartamento and not NomeCategoria:
        breadcrumb_list = soup.find("ul", {"itemtype": "http://schema.org/BreadcrumbList"})
        if breadcrumb_list:
            breadcrumb_items = breadcrumb_list.find_all("li", {"itemprop": "itemListElement"})
            
            # Extrair nomes dos breadcrumbs
            breadcrumb_names = []
            for i, item in enumerate(breadcrumb_items):
                # Primeiro, verificar se tem <strong> (nome do produto)
                strong_tag = item.find("strong")
                if strong_tag:
                    name = limpar(strong_tag.get_text())
                    breadcrumb_names.append(name)
                else:
                    # Buscar o nome dentro do item
                    name_span = item.find("span", {"itemprop": "name"})
                    if name_span:
                        name = limpar(name_span.get_text())
                        breadcrumb_names.append(name)
                    else:
                        # Fallback: buscar em link ou texto direto
                        link = item.find("a")
                        if link:
                            name = limpar(link.get_text())
                            breadcrumb_names.append(name)
                        else:
                            # Texto direto no li
                            text = limpar(item.get_text())
                            if text and text.lower() not in ("voc√™ est√° em:", "you are in:"):
                                breadcrumb_names.append(text)
            
            # Filtrar breadcrumbs v√°lidos (remover "P√°gina Inicial", "In√≠cio", etc.)
            breadcrumb_names = [name for name in breadcrumb_names if name and name.lower() not in ("in√≠cio", "inicio", "home", "p√°gina inicial")]
            
            # Atribuir departamento e categoria
            if len(breadcrumb_names) >= 2:
                NomeDepartamento = breadcrumb_names[-2]  # Pen√∫ltimo item
                NomeCategoria = breadcrumb_names[-1]     # √öltimo item
            elif len(breadcrumb_names) == 1:
                NomeCategoria = breadcrumb_names[0]

    # Fallback: breadcrumb gen√©rico se schema.org n√£o encontrado
    if not NomeDepartamento and not NomeCategoria:
        trail = [limpar(a.get_text()) for a in soup.select(
            "nav.breadcrumb a, .breadcrumb a, .breadcrumbs a, a.breadcrumbs-href, .breadcrumb-item a, .breadcrumb-nav a, [class*='breadcrumb'] a"
        )]
        trail = [t for t in trail if t and t.lower() not in ("in√≠cio", "inicio", "home")]

        nome_h1 = limpar(soup.select_one("div.product-name h1, h1").get_text()) if soup.select_one("div.product-name h1, h1") else ""
        if trail and nome_h1 and trail[-1][:20].lower() in nome_h1[:20].lower():
            trail = trail[:-1]

        # Para Koerich, usar breadcrumb para detectar departamento/categoria
        if trail:
            NomeDepartamento = trail[-2] if len(trail) >= 2 else ""
            NomeCategoria = trail[-1] if len(trail) >= 1 else ""

    # --- Extrair varia√ß√µes de tamanho ---
    tamanhos_disponiveis = []
    
    # Para Koerich (eletrodom√©sticos), geralmente n√£o h√° tamanhos, mas pode haver varia√ß√µes de cor/voltagem
    if "koerich.com.br" in url:
        # Procurar por varia√ß√µes de cor ou voltagem
        variacao_selectors = [
            "select[name*='cor'] option",
            "select[name*='voltagem'] option",
            "select[name*='voltage'] option",
            "[class*='cor'] option",
            "[class*='voltagem'] option",
            "input[name*='cor'][type='radio']",
            "input[name*='voltagem'][type='radio']"
        ]
        
        for selector in variacao_selectors:
            options = soup.select(selector)
            if options:
                for opt in options:
                    variacao = opt.get_text(strip=True) or opt.get("value", "")
                    if variacao and variacao.lower() not in ("selecione", "select", "cor", "voltagem", "-"):
                        tamanhos_disponiveis.append(variacao)
                break
        
        # Se n√£o encontrou varia√ß√µes, usar tamanho √∫nico
        if not tamanhos_disponiveis:
            tamanhos_disponiveis = ["√öNICO"]
    

    
    # Para outros sites, usar tamanho padr√£o
    if not tamanhos_disponiveis:
        tamanhos_disponiveis = ["√öNICO"]

    # Fallback: extrair categoria/departamento do nome do produto
    if not NomeDepartamento or not NomeCategoria:
        nome_lower = nome.lower()
        
            # Para Koerich, detectar eletrodom√©sticos
    if "koerich.com.br" in url:
        # Detectar departamento baseado no tipo de produto
        if any(palavra in nome_lower for palavra in ["frigobar", "freezer", "refrigerador", "geladeira"]):
            NomeDepartamento = "Refrigera√ß√£o"
        elif any(palavra in nome_lower for palavra in ["ar condicionado", "ar-condicionado", "climatizador"]):
            NomeDepartamento = "Ar Condicionado"
        elif any(palavra in nome_lower for palavra in ["ventilador", "ventila√ß√£o", "ventilacao"]):
            NomeDepartamento = "Ventila√ß√£o"
        elif any(palavra in nome_lower for palavra in ["aquecedor", "aquecedores"]):
            NomeDepartamento = "Aquecimento"
        elif any(palavra in nome_lower for palavra in ["m√°quina de lavar", "maquina de lavar", "lavadora"]):
            NomeDepartamento = "Lavagem"
        elif any(palavra in nome_lower for palavra in ["fog√£o", "fogao", "cooktop", "forno"]):
            NomeDepartamento = "Cozinha"
        elif any(palavra in nome_lower for palavra in ["aspirador", "aspiradores"]):
            NomeDepartamento = "Limpeza"
        else:
            NomeDepartamento = "Eletrodom√©sticos"  # Default para Koerich
        
        # Detectar categoria espec√≠fica
        if "frigobar" in nome_lower:
            NomeCategoria = "Frigobar"
        elif "freezer" in nome_lower:
            NomeCategoria = "Freezer"
        elif "refrigerador" in nome_lower or "geladeira" in nome_lower:
            NomeCategoria = "Refrigerador"
        elif "ar condicionado" in nome_lower or "ar-condicionado" in nome_lower:
            NomeCategoria = "Ar Condicionado"
        elif "ventilador" in nome_lower:
            NomeCategoria = "Ventilador"
        elif "aquecedor" in nome_lower:
            NomeCategoria = "Aquecedor"
        elif "m√°quina de lavar" in nome_lower or "maquina de lavar" in nome_lower:
            NomeCategoria = "M√°quina de Lavar"
        elif "fog√£o" in nome_lower or "fogao" in nome_lower:
            NomeCategoria = "Fog√£o"
        elif "microondas" in nome_lower:
            NomeCategoria = "Microondas"
        elif "liquidificador" in nome_lower:
            NomeCategoria = "Liquidificador"
        elif "aspirador" in nome_lower:
            NomeCategoria = "Aspirador"
        else:
            NomeCategoria = "Eletrodom√©sticos"  # Default para Koerich
    else:
        # Para outros sites, usar l√≥gica gen√©rica
        NomeDepartamento = "Eletrodom√©sticos"  # Default gen√©rico
        NomeCategoria = "Eletrodom√©sticos"  # Default gen√©rico

    # SKU: re√∫ne candidatos -> escolhe o 1¬∫ n√£o vazio
    sku_candidates = []
    if isinstance(jsonld, dict) and jsonld.get("sku"):
        sku_candidates.append(str(jsonld["sku"]))
    if nd:
        try:
            prod_nd = nd["props"]["pageProps"]["product"]
            for key in ("itemId", "sku", "id", "productId"):
                v = prod_nd.get(key)
                if v:
                    sku_candidates.append(str(v))
        except Exception:
            pass
    meta_sku = soup.find("meta", {"itemprop": "sku"})
    if meta_sku and meta_sku.get("content"):
        sku_candidates.append(meta_sku["content"].strip())
    # √∫ltimo recurso: slug
    sku_candidates.append(url.rstrip("/").split("/")[-1])
    # Fallback adicional: extrair por r√≥tulo "Ref." ou "Refer√™ncia"
    try:
        full_txt = soup.get_text(" ", strip=True)
        mref = re.search(r"(?:Ref\.?|Refer[e√™]ncia)[:\s]+([A-Z0-9\-\.\/]+)", full_txt, flags=re.I)
        if mref:
            sku_candidates.insert(0, mref.group(1))
    except Exception:
        pass
    sku = next((x for x in sku_candidates if x), "")

    # --- Marca (JSON-LD -> HTML -> Nome do produto) ---
    Marca = ""
    if isinstance(jsonld, dict) and jsonld.get("brand"):
        b = jsonld["brand"]
        if isinstance(b, dict) and b.get("name"): 
            Marca = limpar(b["name"])
        elif isinstance(b, str): 
            Marca = limpar(b)

    if not Marca:
        label = soup.find(string=lambda s: isinstance(s, str) and "Marca" in s)
        if label:
            cont = label.find_parent(["tr","li","p","div","dt","span"])
            if cont:
                dd = cont.find_next_sibling("dd")
                if dd and dd.get_text(strip=True):
                    Marca = limpar(dd.get_text(" ", strip=True))
                else:
                    txt = limpar(cont.get_text(" ", strip=True))
                    m = re.search(r"Marca[:\-]\s*(.+)", txt, flags=re.I)
                    if m: 
                        Marca = limpar(m.group(1))

    # Fallback: extrair marca do nome do produto
    if not Marca:
        nome_lower = nome.lower()
        
        # Para Koerich, detectar marcas de eletrodom√©sticos
        if "koerich.com.br" in url:
            marcas_conhecidas = [
                "midea", "electrolux", "brastemp", "consul", "panasonic", 
                "samsung", "lg", "philco", "ge", "whirlpool", "bosch", 
                "siemens", "fischer", "continental", "cce", "prosd√≥cimo"
            ]
            
            for marca in marcas_conhecidas:
                if marca in nome_lower:
                    Marca = marca.title()
                    break
            
            # Se n√£o encontrou marca espec√≠fica, usar Midea como padr√£o (muito comum na Koerich)
            if not Marca:
                Marca = "Midea"
        
        # Para outros sites, usar l√≥gica gen√©rica
        else:
            marcas_conhecidas = [
                "midea", "electrolux", "brastemp", "consul", "panasonic", 
                "samsung", "lg", "philco", "ge", "whirlpool"
            ]
            
            for marca in marcas_conhecidas:
                if marca in nome_lower:
                    Marca = marca.title()
                    break
            
            # Se n√£o encontrou marca espec√≠fica, usar Koerich como padr√£o
            if not Marca:
                Marca = "Koerich"

    # --- IDs VTEX via mapeamento local ---
    # Para Koerich, usar departamento j√° detectado
    if "koerich.com.br" in url:
        # Departamento e categoria j√° foram detectados acima
        pass
    # Para outros sites, usar l√≥gica gen√©rica
    else:
        # Departamento e categoria j√° foram detectados acima
        pass
    
    _IDDepartamento = maps["departamento"].get(NomeDepartamento, "")
    _IDCategoria = maps["categoria"].get(NomeCategoria, "")
    _IDMarca = get_marca_id(Marca)

    # Imagens: captura de <img> e <source srcset> (t√≠pico em carross√©is)
    # Para Koerich, procurar por imagens espec√≠ficas do produto em alta qualidade
    if "koerich.com.br" in url:
        # Procurar por imagens em carross√©is e galerias
        for img in soup.select("img"):
            src = img.get("src") or img.get("data-src") or img.get("data-lazy-src") or parse_srcset(img.get("srcset"))
            if src and "data:image" not in src and "blank" not in src.lower():
                # Filtrar imagens que parecem ser do produto (cont√™m n√∫meros ou extens√µes de imagem)
                if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']) and any(char.isdigit() for char in src):
                    # Remover par√¢metros de redimensionamento para obter imagem original
                    clean_src = src.split('&')[0] if '&' in src else src
                    imgs.append(clean_src)
        
        # <source srcset> para imagens responsivas - pegar a maior resolu√ß√£o
        for source in soup.select("source"):
            srcset = source.get("srcset")
            if srcset:
                # Parse srcset para pegar a maior resolu√ß√£o
                srcset_parts = srcset.split(',')
                best_url = ""
                best_width = 0
                
                for part in srcset_parts:
                    part = part.strip()
                    if ' ' in part:
                        url_part, width_part = part.rsplit(' ', 1)
                        try:
                            width = int(width_part.replace('w', ''))
                            if width > best_width:
                                best_width = width
                                best_url = url_part.strip()
                        except:
                            pass
                
                if best_url and "data:image" not in best_url:
                    if any(ext in best_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                        # Remover par√¢metros de redimensionamento
                        clean_url = best_url.split('&')[0] if '&' in best_url else best_url
                        imgs.append(clean_url)
        
        # Se n√£o encontrou imagens espec√≠ficas, buscar todas as imagens
        if not imgs:
            for img in soup.select("img"):
                src = img.get("src") or img.get("data-src") or img.get("data-lazy-src") or parse_srcset(img.get("srcset"))
                if src and "data:image" not in src and "blank" not in src.lower():
                    # Remover par√¢metros de redimensionamento
                    clean_src = src.split('&')[0] if '&' in src else src
                    imgs.append(clean_src)
    else:
        # Para outros sites, usar l√≥gica original
        if not imgs:
            # <img>
            for img in soup.select("img"):
                src = img.get("src") or img.get("data-src") or parse_srcset(img.get("srcset"))
                if src and "data:image" not in src and "blank" not in src.lower():
                    imgs.append(src)
            # <source srcset>
            for source in soup.select("source"):
                srcset = source.get("srcset")
                url_first = parse_srcset(srcset)
                if url_first and "data:image" not in url_first:
                    imgs.append(url_first)
        else:
            # Mesmo assim, vamos tentar buscar no HTML para ter mais op√ß√µes
            html_imgs = []
            # <img>
            for img in soup.select("img"):
                src = img.get("src") or img.get("data-src") or parse_srcset(img.get("srcset"))
                if src and "data:image" not in src and "blank" not in src.lower():
                    html_imgs.append(src)
            # <source srcset>
            for source in soup.select("source"):
                srcset = source.get("srcset")
                url_first = parse_srcset(srcset)
                if url_first and "data:image" not in url_first:
                    html_imgs.append(url_first)
            
            # Adicionar imagens do HTML se n√£o estiverem na lista
            for img in html_imgs:
                if img not in imgs:
                    imgs.append(img)

    # Dedup mantendo ordem + normaliza para URL absoluta
    seen, ordered = set(), []
    for u in imgs:
        u_abs = urljoin(url, u)
        if u_abs not in seen:
            seen.add(u_abs)
            ordered.append(u_abs)
    
    # Para Koerich, tentar obter URLs de imagens em alta qualidade
    if "koerich.com.br" in url and ordered:
        high_quality_imgs = []
        for img_url in ordered:
            # Tentar obter vers√£o em alta qualidade
            if "ccstore/v1/images/" in img_url:
                # Remover par√¢metros de redimensionamento
                base_url = img_url.split('?')[0]
                if 'source=' in img_url:
                    # Extrair o caminho original da imagem
                    import urllib.parse
                    parsed = urllib.parse.urlparse(img_url)
                    params = urllib.parse.parse_qs(parsed.query)
                    if 'source' in params:
                        source_path = params['source'][0]
                        # Construir URL de alta qualidade
                        high_quality_url = f"https://www.koerich.com.br{source_path}"
                        high_quality_imgs.append(high_quality_url)
                    else:
                        high_quality_imgs.append(img_url)
                else:
                    high_quality_imgs.append(img_url)
            else:
                high_quality_imgs.append(img_url)
        
        # Usar as URLs de alta qualidade se encontradas
        if high_quality_imgs:
            ordered = high_quality_imgs
    
    # Filtrar apenas imagens do produto (que contenham o SKU)
    imgs_produto = []
    for img in ordered:
        if sku in img or any(sku_part in img for sku_part in sku.split('-')[:2]):
            imgs_produto.append(img)
    
    # Se n√£o encontrou imagens espec√≠ficas do produto, usar as primeiras 5
    if not imgs_produto:
        imgs_produto = ordered[:5]
    
    imgs = imgs_produto[:5]

    # Gerar baseUrl √∫nico para este produto
    base_url_produto = gerar_base_url_produto(sku, nome)
    
    # Baixa at√© 5 imagens
    saved = []
    for i, u in enumerate(imgs, 1):
        fname = f"{sku}_{i}.jpg"
        if baixar_imagem(u, fname):
            saved.append(fname)

    # Gerar m√∫ltiplas linhas para cada tamanho (SKU)
    produtos = []
    for tamanho in tamanhos_disponiveis:
        # SKU √∫nico para cada tamanho
        sku_tamanho = f"{sku}_{tamanho}" if tamanho != "√öNICO" else sku
        nome_tamanho = f"{nome} - {tamanho}" if tamanho != "√öNICO" else nome
        
        produtos.append({
            "_IDSKU": sku_tamanho,
            "_NomeSKU": nome_tamanho,
            "_AtivarSKUSePoss√≠vel": "SIM",
            "_SKUAtivo": "SIM",
            "_EANSKU": "",
            "_Altura": "", "_AlturaReal": "",
            "_Largura": "", "_LarguraReal": "",
            "_Comprimento": "", "_ComprimentoReal": "",
            "_Peso": "", "_PesoReal": "",
            "_UnidadeMedida": "un",
            "_MultiplicadorUnidade": "1,000000",
            "_CodigoReferenciaSKU": sku_tamanho,
            "_ValorFidelidade": "",
            "_DataPrevisaoChegada": "",
            "_CodigoFabricante": "",
            "_IDProduto": sku,  # ID do produto √© o SKU base (sem tamanho)
            "_NomeProduto": nome,  # Nome do produto sem tamanho
            "_BreveDescricaoProduto": (descricao or "")[:200],
            "_ProdutoAtivo": "SIM",
            "_CodigoReferenciaProduto": sku,  # Refer√™ncia do produto √© o SKU base
            "_MostrarNoSite": "SIM",
            "_LinkTexto": url.rstrip("/").split("/")[-1],
            "_DescricaoProduto": descricao or "",
            "_DataLancamentoProduto": datetime.today().strftime("%d/%m/%Y"),
            "_PalavrasChave": "",
            "_TituloSite": nome,
            "_DescricaoMetaTag": (descricao or "")[:160],
            "_IDFornecedor": "",
            "_MostrarSemEstoque": "SIM",
            "_Kit": "",
            "_IDDepartamento": _IDDepartamento,
            "_NomeDepartamento": NomeDepartamento,
            "_IDCategoria": _IDCategoria,
            "_NomeCategoria": NomeCategoria,
            "_IDMarca": _IDMarca,
            "_Marca": Marca,
            "_PesoCubico": "",
            "_Pre√ßo": preco,
            "_BaseUrlImagens": base_url_produto,  # URL base para imagens do produto
            "_ImagensSalvas": ";".join(saved),
            "_ImagensURLs": ";".join(imgs),  # √∫til para POST sku file sem baixar
        })
    
    return produtos

# === Loop principal ===
produtos = []
for _, row in df_links.iterrows():
    url = str(row["url"]).strip()
    if not url:
        continue
    try:
        resultado = extrair_produto(url)
        # resultado pode ser uma lista (m√∫ltiplos SKUs) ou um dict (SKU √∫nico)
        if isinstance(resultado, list):
            produtos.extend(resultado)
        else:
            produtos.append(resultado)
        time.sleep(0.5)  # cortesia para evitar 429
    except Exception as e:
        print(f"‚ùå Erro ao processar {url}: {e}")

# Salvar CSV
df_final = pd.DataFrame(produtos)
df_final.to_csv(output_csv, index=False, encoding="utf-8-sig")

# Mostrar estat√≠sticas
print(f"\n‚úÖ Planilha final salva: {output_csv}")
print(f"üñºÔ∏è Imagens em: {output_folder}")

# Estat√≠sticas de marcas
if len(produtos) > 0:
    marca_counts = df_final['_Marca'].value_counts()
    
    print(f"\nüè∑Ô∏è Marcas encontradas:")
    for marca, count in marca_counts.items():
        marca_id = get_marca_id(marca)
        print(f"   {marca} (ID: {marca_id}): {count} produtos")
    
    print(f"\nüìä Total de marcas √∫nicas: {len(marca_mapping)}")
    print("üìã Mapeamento completo de marcas:")
    for marca, marca_id in sorted(marca_mapping.items()):
        count = marca_counts.get(marca, 0)
        print(f"   {marca} ‚Üí {marca_id} ({count} produtos)")