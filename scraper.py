import os, re, json, time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from pathlib import Path
from tqdm import tqdm

try:
    from playwright.sync_api import sync_playwright
except Exception:
    sync_playwright = None

# === Configura√ß√µes ===
current_dir = os.path.dirname(os.path.abspath(__file__))
input_csv = os.path.join(current_dir, "data", "csv", "produtos_link.csv")
output_csv = os.path.join(current_dir, "data", "exports", "produtos_leo_madeiras.csv")
output_folder = os.path.join(current_dir, "data", "exports", "imagens_produtos")

# Criar pastas necess√°rias
os.makedirs(output_folder, exist_ok=True)

# === Sess√£o HTTP ===
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
})

# === Mapeamentos VTEX ===
maps = {
    "departamento": {
        "MDF": "1", "Madeiras": "2", "Ferramentas El√©tricas": "3",
        "Ferramentas Manuais": "4", "M√°quinas Estacion√°rias": "5",
        "Acess√≥rios para Ferramentas e M√°quinas": "6", "Ferragens": "7",
        "Ferramentas Pneum√°ticas": "8", "Fitas e Tapa Furos": "9",
        "Qu√≠micos": "10", "Perfis de Alum√≠nio": "11", "Ilumina√ß√£o e El√©trica": "12",
        "Revestimentos": "13", "Divis√≥rias": "14", "EPI": "15",
        "Embalagens": "16", "Utilidades Dom√©sticas": "17", "Constru√ß√£o": "18",
        "Cat√°logos e Expositores": "19"
    },
    "categoria": {
        "MDF": "1", "Madeiras": "2", "Furadeira": "3", "Parafusadeira": "4",
        "Furadeira de Impacto": "5", "Martelete": "6", "Serra Circular": "7",
        "Serra Meia-Esquadria": "8", "Serra de Bancada": "9", "Serra Tico Tico": "10",
        "Serra M√°rmore": "11", "Plaina": "12", "Pinador": "13", "Esmerilhadeira": "14",
        "Linha Laser": "15", "Soprador T√©rmico": "16", "Chave de Impacto": "17",
        "Tupia": "18", "Tico Tico de Bancada": "19"
    }
}

# === Fun√ß√µes Utilit√°rias ===
def limpar(texto):
    return re.sub(r"\s+", " ", (texto or "").strip())

def get_marca_id(marca_nome):
    """Retorna ID da marca no formato 2000XXX"""
    if not marca_nome:
        return "2000001"
    
    marca_upper = marca_nome.upper().strip()
    # Mapeamento simples de marcas conhecidas
    marcas_ids = {
        "KRESS": "2000001", "BOSCH": "2000002", "MAKITA": "2000003",
        "DEWALT": "2000004", "MILWAUKEE": "2000005", "BLACK+DECKER": "2000006",
        "STANLEY": "2000007", "METABO": "2000008", "HITACHI": "2000009",
        "PANASONIC": "2000010", "RYOBI": "2000011"
    }
    
    return marcas_ids.get(marca_upper, "2000001")

def parse_preco(texto):
    """Extrai pre√ßo de texto com formato brasileiro"""
    if not texto:
        return ""
    
    # Padr√µes de pre√ßo
    patterns = [
        r"R\$\s*([\d\.\s]+,\d{2})",
        r"([\d\.\s]+,\d{2})",
        r"([\d]+\.\d{2})"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, texto)
        if match:
            price_str = match.group(1).strip()
            try:
                price_clean = re.sub(r'[^\d,.]', '', price_str)
                if ',' in price_clean and '.' in price_clean:
                    price_clean = price_clean.replace('.', '').replace(',', '.')
                elif ',' in price_clean:
                    price_clean = price_clean.replace(',', '.')
                
                return f"{float(price_clean):.2f}"
            except:
                continue
    return ""

def renderizar_html(url):
    """Renderiza p√°gina via Playwright"""
    if not sync_playwright:
        print("‚ö†Ô∏è Playwright n√£o dispon√≠vel, usando HTML est√°tico")
        r = session.get(url, timeout=20)
        return r.text
    
    try:
        p = sync_playwright().start()
        browser = p.chromium.launch(headless=True)
        context = browser.new_context()
        page = context.new_page()
        page.goto(url, wait_until="domcontentloaded")
        page.wait_for_load_state("networkidle", timeout=15000)
        html = page.content()
        browser.close()
        p.stop()
        return html
    except Exception as e:
        print(f"‚ö†Ô∏è Erro com Playwright: {e}")
        r = session.get(url, timeout=20)
        return r.text

def baixar_imagem(url_img, fname):
    """Baixa imagem do produto"""
    try:
        with session.get(url_img, stream=True, timeout=30) as resp:
            resp.raise_for_status()
            with open(os.path.join(output_folder, fname), "wb") as f:
                for chunk in resp.iter_content(8192):
                    if chunk:
                        f.write(chunk)
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao baixar {url_img}: {e}")
        return False

# === Fun√ß√£o Principal ===
def extrair_produto(url):
    """Extrai dados do produto da Leo Madeiras"""
    print(f"üîç Processando: {url}")
    
    # Renderizar p√°gina
    html = renderizar_html(url)
    soup = BeautifulSoup(html, "html.parser")
    
    # === Extrair Nome ===
    nome = ""
    for sel in [".product-name h1", "h1.product-name", "h1", ".product-title"]:
        tag = soup.select_one(sel)
        if tag and tag.get_text(strip=True):
            nome_temp = limpar(tag.get_text(strip=True))
            if (nome_temp and 
                nome_temp.lower() not in ["onde voc√™ est√°?", "onde voce esta?", "navega√ß√£o"] and
                len(nome_temp) > 5):
                nome = nome_temp
                break
    
    if not nome:
        # Extrair da URL
        url_parts = url.rstrip("/").split("/")
        if len(url_parts) >= 2:
            nome = url_parts[-1].replace("-", " ").title()
    
    if not nome:
        nome = "Sem Nome"
    
    print(f"‚úÖ Nome: {nome}")
    
    # === Extrair Pre√ßo ===
    preco = ""
    
    # 1. data-price
    for element in soup.select("[data-price]"):
        data_price = element.get("data-price")
        if data_price:
            try:
                preco = f"{float(str(data_price).replace(',', '.')):.2f}"
                print(f"‚úÖ Pre√ßo via data-price: {preco}")
                break
            except:
                pass
    
    # 2. data-sku-obj
    if not preco:
        for element in soup.select("[data-sku-obj]"):
            data_sku_obj = element.get("data-sku-obj")
            if data_sku_obj:
                try:
                    import html
                    decoded = html.unescape(data_sku_obj)
                    sku_data = json.loads(decoded)
                    
                    if "price" in sku_data:
                        preco = f"{float(str(sku_data['price']).replace(',', '.')):.2f}"
                        print(f"‚úÖ Pre√ßo via data-sku-obj: {preco}")
                        break
                    elif "best" in sku_data and "price" in sku_data["best"]:
                        preco = f"{float(str(sku_data['best']['price']).replace(',', '.')):.2f}"
                        print(f"‚úÖ Pre√ßo via data-sku-obj.best: {preco}")
                        break
                except:
                    continue
    
    # 3. Fallback: regex no texto
    if not preco:
        preco = parse_preco(soup.get_text(" ", strip=True))
        if preco:
            print(f"‚úÖ Pre√ßo via regex: {preco}")
    
    if not preco:
        preco = "0.00"
        print("‚ö†Ô∏è Pre√ßo n√£o encontrado")
    
    # === Extrair SKU ===
    sku = ""
    url_parts = url.rstrip("/").split("/")
    if len(url_parts) >= 2:
        sku = url_parts[-2]
    
    if not sku:
        sku = "SKU_" + str(int(time.time()))
    
    # === Extrair Marca ===
    marca = "Leo Madeiras"
    nome_lower = nome.lower()
    marcas_conhecidas = ["kress", "bosch", "makita", "dewalt", "milwaukee"]
    
    for marca_conhecida in marcas_conhecidas:
        if marca_conhecida in nome_lower:
            marca = marca_conhecida.title()
            break
    
    # === Detectar Categoria/Departamento ===
    nome_lower = nome.lower()
    
    if any(palavra in nome_lower for palavra in ["furadeira", "parafusadeira", "martelete", "serra"]):
        departamento = "Ferramentas El√©tricas"
        categoria = "Furadeira" if "furadeira" in nome_lower else "Parafusadeira"
    elif any(palavra in nome_lower for palavra in ["mdf", "madeira"]):
        departamento = "Madeiras"
        categoria = "MDF" if "mdf" in nome_lower else "Madeiras"
    else:
        departamento = "Ferramentas El√©tricas"
        categoria = "Ferramentas El√©tricas"
    
    # === Extrair Imagens ===
    imgs = []
    for img in soup.select("img"):
        src = img.get("src") or img.get("data-src")
        if src and "data:image" not in src and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png']):
            imgs.append(urljoin(url, src))
    
    # Baixar imagens
    saved = []
    if imgs:
        print(f"üì∏ Baixando {len(imgs[:5])} imagens...")
        with tqdm(total=len(imgs[:5]), desc="üñºÔ∏è Download imagens", 
                  bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as img_pbar:
            
            for i, img_url in enumerate(imgs[:5], 1):
                fname = f"{sku}_{i}.jpg"
                img_pbar.set_description(f"üì• Baixando {fname}")
                
                if baixar_imagem(img_url, fname):
                    saved.append(fname)
                    img_pbar.set_postfix({'Status': '‚úÖ Sucesso'})
                else:
                    img_pbar.set_postfix({'Status': '‚ùå Falha'})
                
                img_pbar.update(1)
    else:
        print("‚ö†Ô∏è Nenhuma imagem encontrada para download")
    
    # === Gerar Produto ===
    produto = {
        "_IDSKU": sku,
        "_NomeSKU": nome,
        "_AtivarSKUSePoss√≠vel": "SIM",
        "_SKUAtivo": "SIM",
        "_EANSKU": "",
        "_Altura": "", "_AlturaReal": "",
        "_Largura": "", "_LarguraReal": "",
        "_Comprimento": "", "_ComprimentoReal": "",
        "_Peso": "", "_PesoReal": "",
        "_UnidadeMedida": "un",
        "_MultiplicadorUnidade": "1,000000",
        "_CodigoReferenciaSKU": sku,
        "_ValorFidelidade": "",
        "_DataPrevisaoChegada": "",
        "_CodigoFabricante": "",
        "_IDProduto": sku,
        "_NomeProduto": nome,
        "_BreveDescricaoProduto": nome[:200],
        "_ProdutoAtivo": "SIM",
        "_CodigoReferenciaProduto": sku,
        "_MostrarNoSite": "SIM",
        "_LinkTexto": url.rstrip("/").split("/")[-1],
        "_DescricaoProduto": nome,
        "_DataLancamentoProduto": datetime.today().strftime("%d/%m/%Y"),
        "_PalavrasChave": "",
        "_TituloSite": nome,
        "_DescricaoMetaTag": nome[:160],
        "_IDFornecedor": "",
        "_MostrarSemEstoque": "SIM",
        "_Kit": "",
        "_IDDepartamento": maps["departamento"].get(departamento, ""),
        "_NomeDepartamento": departamento,
        "_IDCategoria": maps["categoria"].get(categoria, ""),
        "_NomeCategoria": categoria,
        "_IDMarca": get_marca_id(marca),
        "_Marca": marca,
        "_PesoCubico": "",
        "_Pre√ßo": preco,
        "_BaseUrlImagens": f"images-leo-madeiras-{sku}",
        "_ImagensSalvas": ";".join(saved),
        "_ImagensURLs": ";".join(imgs),
    }
    
    return produto

# === Execu√ß√£o Principal ===
if __name__ == "__main__":
    # Ler CSV de entrada
    try:
        df_links = pd.read_csv(input_csv)
        if "url" not in df_links.columns:
            raise Exception("‚ùå A planilha precisa ter uma coluna chamada 'url'.")
    except Exception as e:
        print(f"‚ùå Erro ao ler CSV: {e}")
        exit(1)
    
    # Processar produtos
    produtos = []
    
    # Filtrar apenas URLs v√°lidas da Leo Madeiras
    urls_validas = []
    for _, row in df_links.iterrows():
        url = str(row["url"]).strip()
        if url and "leomadeiras.com.br" in url:
            urls_validas.append(url)
    
    if not urls_validas:
        print("‚ùå Nenhuma URL v√°lida da Leo Madeiras encontrada")
        exit(1)
    
    print(f"üöÄ Iniciando processamento de {len(urls_validas)} produtos...")
    
    # Barra de progresso principal
    with tqdm(total=len(urls_validas), desc="üîÑ Scraping produtos", 
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
        
        for url in urls_validas:
            try:
                # Atualizar descri√ß√£o da barra
                pbar.set_description(f"üîç Processando: {url.split('/')[-1][:30]}...")
                
                resultado = extrair_produto(url)
                if resultado:
                    produtos.append(resultado)
                    pbar.set_postfix({
                        'SKU': resultado['_IDSKU'],
                        'Pre√ßo': f"R$ {resultado['_Pre√ßo']}",
                        'Marca': resultado['_Marca']
                    })
                else:
                    pbar.set_postfix({'Erro': 'Falha na extra√ß√£o'})
                
                time.sleep(1)  # Cortesia
                pbar.update(1)
                
            except Exception as e:
                pbar.set_postfix({'Erro': str(e)[:20]})
                pbar.update(1)
                continue
    
    # Salvar resultados
    if produtos:
        df_final = pd.DataFrame(produtos)
        df_final.to_csv(output_csv, index=False, encoding="utf-8-sig")
        
        print(f"\n‚úÖ Planilha salva: {output_csv}")
        print(f"üñºÔ∏è Imagens em: {output_folder}")
        print(f"üìä Total processados: {len(produtos)}")
        
        # Estat√≠sticas
        marca_counts = df_final['_Marca'].value_counts()
        print(f"\nüè∑Ô∏è Marcas encontradas:")
        for marca, count in marca_counts.items():
            print(f"   {marca}: {count} produtos")
    else:
        print("‚ùå Nenhum produto processado")