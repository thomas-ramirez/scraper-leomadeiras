import os, re, json, time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from pathlib import Path
from tqdm import tqdm

try:
    from playwright.sync_api import sync_playwright
except Exception:
    sync_playwright = None

# === Configura√ß√µes ===
current_dir = os.path.dirname(os.path.abspath(__file__))
input_csv = os.path.join(current_dir, "data", "csv", "produtos_link.csv")
output_csv = os.path.join(current_dir, "data", "exports", "produtos_leo_madeiras.csv")
output_folder = os.path.join(current_dir, "data", "exports", "imagens_produtos")

# Criar pastas necess√°rias
os.makedirs(output_folder, exist_ok=True)

# === Sess√£o HTTP Otimizada ===
session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Referer": "https://www.leomadeiras.com.br/"
})

# Configura√ß√µes de performance para conex√µes HTTP
from requests.adapters import HTTPAdapter
session.mount('http://', HTTPAdapter(
    pool_connections=10,
    pool_maxsize=20,
    max_retries=1
))
session.mount('https://', HTTPAdapter(
    pool_connections=10,
    pool_maxsize=20,
    max_retries=1
))

# === Mapeamentos VTEX ===
maps = {
    "departamento": {
        "MDF": "1", "Madeiras": "2", "Ferramentas El√©tricas": "3",
        "Ferramentas Manuais": "4", "M√°quinas Estacion√°rias": "5",
        "Acess√≥rios para Ferramentas e M√°quinas": "6", "Ferragens": "7",
        "Ferramentas Pneum√°ticas": "8", "Fitas e Tapa Furos": "9",
        "Qu√≠micos": "10", "Perfis de Alum√≠nio": "11", "Ilumina√ß√£o e El√©trica": "12",
        "Revestimentos": "13", "Divis√≥rias": "14", "EPI": "15",
        "Embalagens": "16", "Utilidades Dom√©sticas": "17", "Constru√ß√£o": "18",
        "Cat√°logos e Expositores": "19"
    },
    "categoria": {
        "MDF": "1", "Madeiras": "2", "Furadeira": "3", "Parafusadeira": "4",
        "Furadeira de Impacto": "5", "Martelete": "6", "Serra Circular": "7",
        "Serra Meia-Esquadria": "8", "Serra de Bancada": "9", "Serra Tico Tico": "10",
        "Serra M√°rmore": "11", "Plaina": "12", "Pinador": "13", "Esmerilhadeira": "14",
        "Linha Laser": "15", "Soprador T√©rmico": "16", "Chave de Impacto": "17",
        "Tupia": "18", "Tico Tico de Bancada": "19"
    }
}

# === Fun√ß√µes Utilit√°rias ===
def limpar(texto):
    return re.sub(r"\s+", " ", (texto or "").strip())

def get_marca_id(marca_nome):
    """Retorna ID da marca no formato 2000XXX"""
    if not marca_nome:
        return "2000001"
    
    marca_upper = marca_nome.upper().strip()
    # Mapeamento simples de marcas conhecidas
    marcas_ids = {
        "KRESS": "2000001", "BOSCH": "2000002", "MAKITA": "2000003",
        "DEWALT": "2000004", "MILWAUKEE": "2000005", "BLACK+DECKER": "2000006",
        "STANLEY": "2000007", "METABO": "2000008", "HITACHI": "2000009",
        "PANASONIC": "2000010", "RYOBI": "2000011"
    }
    
    return marcas_ids.get(marca_upper, "2000001")

def parse_preco(texto):
    """Extrai pre√ßo de texto com formato brasileiro"""
    if not texto:
        return ""
    
    # Padr√µes de pre√ßo
    patterns = [
        r"R\$\s*([\d\.\s]+,\d{2})",
        r"([\d\.\s]+,\d{2})",
        r"([\d]+\.\d{2})"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, texto)
        if match:
            price_str = match.group(1).strip()
            try:
                price_clean = re.sub(r'[^\d,.]', '', price_str)
                if ',' in price_clean and '.' in price_clean:
                    price_clean = price_clean.replace('.', '').replace(',', '.')
                elif ',' in price_clean:
                    price_clean = price_clean.replace(',', '.')
                
                return f"{float(price_clean):.2f}"
            except:
                continue
    return ""

# Vari√°veis globais para reutilizar browser
_playwright_instance = None
_browser = None
_context = None

def get_playwright_instance():
    """Retorna inst√¢ncia reutiliz√°vel do Playwright"""
    global _playwright_instance, _browser, _context
    
    if _playwright_instance is None and sync_playwright is not None:
        _playwright_instance = sync_playwright().start()
        _browser = _playwright_instance.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        )
        _context = _browser.new_context(
            viewport={'width': 1280, 'height': 720},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
    
    return _playwright_instance, _browser, _context

def cleanup_playwright():
    """Limpa recursos do Playwright"""
    global _playwright_instance, _browser, _context
    
    if _context:
        _context.close()
    if _browser:
        _browser.close()
    if _playwright_instance:
        _playwright_instance.stop()
    
    _playwright_instance = None
    _browser = None
    _context = None

def renderizar_html(url):
    """Renderiza p√°gina via Playwright com otimiza√ß√µes"""
    if not sync_playwright:
        print("‚ö†Ô∏è Playwright n√£o dispon√≠vel, usando HTML est√°tico")
        r = session.get(url, timeout=10)
        return r.text
    
    try:
        _, _, context = get_playwright_instance()
        if context is None:
            raise Exception("Context n√£o dispon√≠vel")
            
        page = context.new_page()
        
        # Otimiza√ß√µes de performance
        page.set_default_timeout(15000)  # Aumentado para 15s para p√°ginas complexas
        page.set_default_navigation_timeout(15000)
        
        # N√ÉO desabilitar JavaScript - precisamos dele para carregar as imagens
        page.route("**/*.{png,jpg,jpeg,gif,svg,woff,woff2,ttf,eot}", lambda route: route.abort())
        # page.route("**/*.{css,js}", lambda route: route.abort())  # Comentado para permitir JS
        
        page.goto(url, wait_until="domcontentloaded")
        page.wait_for_load_state("domcontentloaded", timeout=10000)  # Aumentado para 10s
        
        # Aguardar um pouco mais para JavaScript carregar as imagens
        page.wait_for_timeout(2000)
        
        html = page.content()
        page.close()
        return html
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erro com Playwright: {e}")
        r = session.get(url, timeout=10)
        return r.text

def baixar_imagem(url_img, fname):
    """Baixa imagem do produto com otimiza√ß√µes"""
    try:
        with session.get(url_img, stream=True, timeout=15) as resp:  # Reduzido de 30s para 15s
            resp.raise_for_status()
            with open(os.path.join(output_folder, fname), "wb") as f:
                for chunk in resp.iter_content(16384):  # Aumentado de 8KB para 16KB
                    if chunk:
                        f.write(chunk)
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è Erro ao baixar {url_img}: {e}")
        return False

# === Fun√ß√£o Principal ===
def extrair_produto(url):
    """Extrai dados do produto da Leo Madeiras"""
    print(f"üîç Processando: {url}")
    
    # Renderizar p√°gina
    html = renderizar_html(url)
    soup = BeautifulSoup(html, "html.parser")
    
    # === Extrair Nome ===
    nome = ""
    for sel in [".product-name h1", "h1.product-name", "h1", ".product-title"]:
        tag = soup.select_one(sel)
        if tag and tag.get_text(strip=True):
            nome_temp = limpar(tag.get_text(strip=True))
            if (nome_temp and 
                nome_temp.lower() not in ["onde voc√™ est√°?", "onde voce esta?", "navega√ß√£o"] and
                len(nome_temp) > 5):
                nome = nome_temp
                break
    
    if not nome:
        # Extrair da URL
        url_parts = url.rstrip("/").split("/")
        if len(url_parts) >= 2:
            nome = url_parts[-1].replace("-", " ").title()
    
    if not nome:
        nome = "Sem Nome"
    
    print(f"‚úÖ Nome: {nome}")
    
    # === Extrair Descri√ß√£o ===
    descricao = ""
    
    # 1. Tentar extrair da se√ß√£o de descri√ß√£o do produto
    descricao_selectors = [
        ".product-description",
        ".product-details",
        ".description",
        ".produto-descricao",
        ".descricao-produto",
        "[data-description]",
        ".product-info .description",
        ".product-content .description"
    ]
    
    for selector in descricao_selectors:
        desc_tag = soup.select_one(selector)
        if desc_tag:
            desc_text = desc_tag.get_text(" ", strip=True)
            if desc_text and len(desc_text) > 50:  # Descri√ß√£o deve ter pelo menos 50 caracteres
                descricao = limpar(desc_text)
                print(f"‚úÖ Descri√ß√£o encontrada via selector: {selector}")
                break
    
    # 2. Tentar extrair de elementos com texto descritivo
    if not descricao:
        for tag in soup.find_all(["p", "div", "span"]):
            if tag.get_text(strip=True):
                text = tag.get_text(strip=True)
                # Verificar se parece uma descri√ß√£o de produto
                if (len(text) > 100 and 
                    any(keyword in text.lower() for keyword in ["aplica√ß√µes", "benef√≠cios", "caracter√≠sticas", "especifica√ß√µes", "detalhes", "informa√ß√µes"])):
                    descricao = limpar(text)
                    print(f"‚úÖ Descri√ß√£o encontrada via texto descritivo")
                    break
    
    # 3. Fallback: usar nome do produto
    if not descricao:
        descricao = nome
        print(f"‚ö†Ô∏è Descri√ß√£o n√£o encontrada, usando nome do produto")
    
    print(f"üìù Descri√ß√£o extra√≠da: {descricao[:100]}...")
    
    # === Extrair Pre√ßo ===
    preco = ""
    
    # 1. data-price
    for element in soup.select("[data-price]"):
        data_price = element.get("data-price")
        if data_price:
            try:
                preco = f"{float(str(data_price).replace(',', '.')):.2f}"
                print(f"‚úÖ Pre√ßo via data-price: {preco}")
                break
            except:
                pass
    
    # 2. data-sku-obj
    if not preco:
        for element in soup.select("[data-sku-obj]"):
            data_sku_obj = element.get("data-sku-obj")
            if data_sku_obj:
                try:
                    import html
                    if isinstance(data_sku_obj, str):
                        decoded = html.unescape(data_sku_obj)
                        sku_data = json.loads(decoded)
                        
                        if "price" in sku_data:
                            preco = f"{float(str(sku_data['price']).replace(',', '.')):.2f}"
                            print(f"‚úÖ Pre√ßo via data-sku-obj: {preco}")
                            break
                        elif "best" in sku_data and "price" in sku_data["best"]:
                            preco = f"{float(str(sku_data['best']['price']).replace(',', '.')):.2f}"
                            print(f"‚úÖ Pre√ßo via data-sku-obj.best: {preco}")
                            break
                except:
                    continue
    
    # 3. Fallback: regex no texto
    if not preco:
        preco = parse_preco(soup.get_text(" ", strip=True))
        if preco:
            print(f"‚úÖ Pre√ßo via regex: {preco}")
    
    if not preco:
        preco = "0.00"
        print("‚ö†Ô∏è Pre√ßo n√£o encontrado")
    
    # === Extrair SKU ===
    sku = ""
    url_parts = url.rstrip("/").split("/")
    if len(url_parts) >= 2:
        sku = url_parts[-2]
    
    if not sku:
        sku = "SKU_" + str(int(time.time()))
    
    # === Extrair Marca ===
    marca = "Leo Madeiras"
    nome_lower = nome.lower()
    marcas_conhecidas = ["kress", "bosch", "makita", "dewalt", "milwaukee"]
    
    for marca_conhecida in marcas_conhecidas:
        if marca_conhecida in nome_lower:
            marca = marca_conhecida.title()
            break
    
    # === Detectar Categoria/Departamento ===
    nome_lower = nome.lower()
    
    if any(palavra in nome_lower for palavra in ["furadeira", "parafusadeira", "martelete", "serra"]):
        departamento = "Ferramentas El√©tricas"
        categoria = "Furadeira" if "furadeira" in nome_lower else "Parafusadeira"
    elif any(palavra in nome_lower for palavra in ["mdf", "madeira"]):
        departamento = "Madeiras"
        categoria = "MDF" if "mdf" in nome_lower else "Madeiras"
    else:
        departamento = "Ferramentas El√©tricas"
        categoria = "Ferramentas El√©tricas"
    
    # === Extrair Imagens ===
    imgs = []
    
    # Buscar especificamente por imagens de produtos nos elementos com zoom
    # Prioridade 1: Imagens dentro de divs com data-zoom-image (mais confi√°vel)
    for zoom_div in soup.select("div[data-zoom-image]"):
        zoom_img_url = zoom_div.get("data-zoom-image")
        if zoom_img_url and isinstance(zoom_img_url, str) and "cws.digital" in zoom_img_url:
            # Verificar se √© uma imagem v√°lida
            if any(ext in zoom_img_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                imgs.append(zoom_img_url)
                print(f"‚úÖ Imagem encontrada via data-zoom-image: {zoom_img_url}")
    
    # Prioridade 2: Imagens dentro de divs com classe "zoom" ou similares
    for zoom_div in soup.select("div.zoom, div[class*='zoom'], div[class*='image']"):
        for img in zoom_div.select("img"):
            src = img.get("src") or img.get("data-src")
            if src and isinstance(src, str) and "cws.digital" in src:
                if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    if src not in imgs:  # Evitar duplicatas
                        imgs.append(src)
                        print(f"‚úÖ Imagem encontrada via div zoom: {src}")
    
    # Prioridade 3: Imagens com classe "zoomImg" (imagens de zoom dos produtos)
    for img in soup.select("img.zoomImg"):
        src = img.get("src") or img.get("data-src")
        if src and isinstance(src, str) and "cws.digital" in src:
            if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                if src not in imgs:  # Evitar duplicatas
                    imgs.append(src)
                    print(f"‚úÖ Imagem encontrada via classe zoomImg: {src}")
    
    # Prioridade 4: Imagens com classe "original" (geralmente s√£o as principais)
    for img in soup.select("img.original"):
        src = img.get("src") or img.get("data-src")
        if src and isinstance(src, str) and "cws.digital" in src:
            if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                if src not in imgs:  # Evitar duplicatas
                    imgs.append(src)
                    print(f"‚úÖ Imagem encontrada via classe original: {src}")
    
    # Prioridade 4: Buscar por imagens espec√≠ficas de produtos (mais rigoroso)
    if not imgs:
        for img in soup.select("img"):
            src = img.get("src") or img.get("data-src")
            if not src or "data:image" in src:
                continue
                
            if isinstance(src, str):
                src_lower = src.lower()
                
                # Verificar extens√£o de imagem
                if not any(ext in src_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    continue
                
                # Verificar se √© uma imagem de produto (deve conter "/produtos/" e ser do dom√≠nio correto)
                if ("/produtos/" in src_lower and 
                    "cws.digital" in src_lower and
                    not any(exclude in src_lower for exclude in ["/multimidia/", "/fornecedores/", "instagram", "facebook", "linkedln"])):
                    
                    if src not in imgs:  # Evitar duplicatas
                        imgs.append(src)
                        print(f"‚úÖ Imagem encontrada via padr√£o produto: {src}")
                        if len(imgs) >= 5:  # Limite de 5 imagens
                            break
    
    # Prioridade 5: Buscar por imagens que contenham o SKU espec√≠fico (√∫ltimo recurso)
    if not imgs:
        for img in soup.select("img"):
            src = img.get("src") or img.get("data-src")
            if not src or "data:image" in src:
                continue
                
            if isinstance(src, str):
                src_lower = src.lower()
                
                # Verificar extens√£o de imagem
                if not any(ext in src_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    continue
                
                # Verificar se cont√©m SKU espec√≠fico
                if sku in src_lower:
                    if src not in imgs:  # Evitar duplicatas
                        imgs.append(src)
                        print(f"‚úÖ Imagem encontrada via SKU: {src}")
                        if len(imgs) >= 5:  # Limite de 5 imagens
                            break
    
    # Prioridade 6: Buscar por imagens que seguem o padr√£o dos produtos que funcionam
    if not imgs:
        print(f"üîç Buscando por imagens com padr√£o de produto...")
        
        # Buscar por qualquer imagem que contenha "/produtos/" e seja do dom√≠nio correto
        for img in soup.select("img"):
            src = img.get("src") or img.get("data-src")
            if not src or "data:image" in src:
                continue
                
            if isinstance(src, str):
                src_lower = src.lower()
                
                # Verificar extens√£o de imagem
                if not any(ext in src_lower for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    continue
                
                # Verificar se √© uma imagem de produto (deve conter "/produtos/" e ser do dom√≠nio correto)
                if ("/produtos/" in src_lower and 
                    "cws.digital" in src_lower and
                    not any(exclude in src_lower for exclude in ["/multimidia/", "/fornecedores/", "instagram", "facebook", "linkedln", "youtube", "pinterest", "tiktok"])):
                    
                    if src not in imgs:  # Evitar duplicatas
                        imgs.append(src)
                        print(f"‚úÖ Imagem encontrada via padr√£o produto: {src}")
                        if len(imgs) >= 5:  # Limite de 5 imagens
                            break
    
    # Remover duplicatas e limitar a 5 imagens
    imgs = list(dict.fromkeys(imgs))[:5]  # dict.fromkeys preserva ordem e remove duplicatas
    
    print(f"üì∏ Encontradas {len(imgs)} imagens do produto (SKU: {sku})")
    
    # Baixar imagens
    saved = []
    if imgs:
        print(f"üì∏ Baixando {len(imgs)} imagens...")
        with tqdm(total=len(imgs), desc="üñºÔ∏è Download imagens", 
                  bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as img_pbar:
            
            for i, img_url in enumerate(imgs, 1):
                fname = f"{sku}_{i}.jpg"
                img_pbar.set_description(f"üì• Baixando {fname}")
                
                # As URLs j√° s√£o completas, n√£o precisamos de urljoin
                if baixar_imagem(img_url, fname):
                    saved.append(fname)
                    img_pbar.set_postfix({'Status': '‚úÖ Sucesso'})
                else:
                    img_pbar.set_postfix({'Status': '‚ùå Falha'})
                
                img_pbar.update(1)
    else:
        print("‚ö†Ô∏è Nenhuma imagem encontrada para download")
    
    # === Gerar Produto ===
    produto = {
        "_IDSKU": sku,
        "_NomeSKU": nome,
        "_AtivarSKUSePoss√≠vel": "SIM",
        "_SKUAtivo": "SIM",
        "_EANSKU": "",
        "_Altura": "", "_AlturaReal": "",
        "_Largura": "", "_LarguraReal": "",
        "_Comprimento": "", "_ComprimentoReal": "",
        "_Peso": "", "_PesoReal": "",
        "_UnidadeMedida": "un",
        "_MultiplicadorUnidade": "1,000000",
        "_CodigoReferenciaSKU": sku,
        "_ValorFidelidade": "",
        "_DataPrevisaoChegada": "",
        "_CodigoFabricante": "",
        "_IDProduto": sku,
        "_NomeProduto": nome,
        "_BreveDescricaoProduto": descricao[:200] if descricao else nome[:200],
        "_ProdutoAtivo": "SIM",
        "_CodigoReferenciaProduto": sku,
        "_MostrarNoSite": "SIM",
        "_LinkTexto": url.rstrip("/").split("/")[-1],
        "_DescricaoProduto": descricao if descricao else nome,
        "_DataLancamentoProduto": datetime.today().strftime("%d/%m/%Y"),
        "_PalavrasChave": "",
        "_TituloSite": nome,
        "_DescricaoMetaTag": nome[:160],
        "_IDFornecedor": "",
        "_MostrarSemEstoque": "SIM",
        "_Kit": "",
        "_IDDepartamento": maps["departamento"].get(departamento, ""),
        "_NomeDepartamento": departamento,
        "_IDCategoria": maps["categoria"].get(categoria, ""),
        "_NomeCategoria": categoria,
        "_IDMarca": get_marca_id(marca),
        "_Marca": marca,
        "_PesoCubico": "",
        "_Pre√ßo": preco,
        "_BaseUrlImagens": f"images-leo-madeiras-{sku}",
        "_ImagensSalvas": ";".join(saved),
        "_ImagensURLs": ";".join(imgs),
    }
    
    return produto

# === Execu√ß√£o Principal ===
if __name__ == "__main__":
    # Ler CSV de entrada
    try:
        df_links = pd.read_csv(input_csv)
        if "url" not in df_links.columns:
            raise Exception("‚ùå A planilha precisa ter uma coluna chamada 'url'.")
    except Exception as e:
        print(f"‚ùå Erro ao ler CSV: {e}")
        exit(1)
    
    # Processar produtos
    produtos = []
    
    # Filtrar apenas URLs v√°lidas da Leo Madeiras
    urls_validas = []
    for _, row in df_links.iterrows():
        url = str(row["url"]).strip()
        if url and "leomadeiras.com.br" in url:
            urls_validas.append(url)
    
    if not urls_validas:
        print("‚ùå Nenhuma URL v√°lida da Leo Madeiras encontrada")
        exit(1)
    
    print(f"üöÄ Iniciando processamento de {len(urls_validas)} produtos...")
    
    # Barra de progresso principal
    with tqdm(total=len(urls_validas), desc="üîÑ Scraping produtos", 
              bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]') as pbar:
        
        for url in urls_validas:
            try:
                # Atualizar descri√ß√£o da barra
                pbar.set_description(f"üîç Processando: {url.split('/')[-1][:30]}...")
                
                resultado = extrair_produto(url)
                if resultado:
                    produtos.append(resultado)
                    pbar.set_postfix({
                        'SKU': resultado['_IDSKU'],
                        'Pre√ßo': f"R$ {resultado['_Pre√ßo']}",
                        'Marca': resultado['_Marca']
                    })
                else:
                    pbar.set_postfix({'Erro': 'Falha na extra√ß√£o'})

                
                pbar.update(1)
                
            except Exception as e:
                pbar.set_postfix({'Erro': str(e)[:20]})
                pbar.update(1)
                continue
    
    # Salvar resultados
    if produtos:
        df_final = pd.DataFrame(produtos)
        df_final.to_csv(output_csv, index=False, encoding="utf-8-sig")
        
        print(f"\n‚úÖ Planilha salva: {output_csv}")
        print(f"üñºÔ∏è Imagens em: {output_folder}")
        print(f"üìä Total processados: {len(produtos)}")
        
        # Estat√≠sticas
        marca_counts = df_final['_Marca'].value_counts()
        print(f"\nüè∑Ô∏è Marcas encontradas:")
        for marca, count in marca_counts.items():
            print(f"   {marca}: {count} produtos")
    else:
        print("‚ùå Nenhum produto processado")
    
    # Limpar recursos do Playwright
    cleanup_playwright()